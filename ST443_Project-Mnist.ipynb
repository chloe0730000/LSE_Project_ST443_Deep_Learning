{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Kaggle sample:\n",
    "https://www.kaggle.com/kakauandme/tensorflow-deep-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf   # pip install tensorflow  \n",
    "\n",
    "# settings\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# set to 20000 on local environment to get 0.99 accuracy\n",
    "TRAINING_ITERATIONS = 10000        \n",
    "    \n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "# image number to output\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "#path=\"/Users/chloe/Desktop/ST445_Project\"\n",
    "path='/Users/lin/Desktop/ST443_Project_Data/Data' # another laptop\n",
    "os.chdir(path)\n",
    "data = pd.read_csv('mnist_train.csv', sep=',') \n",
    "#test = pd.read_csv('test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(59999,785)\n"
     ]
    }
   ],
   "source": [
    "print('data({0[0]},{0[1]})'.format(data.shape)) # (59999, 785) -> include one label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "(59999, 784)\n"
     ]
    }
   ],
   "source": [
    "images = data.iloc[:,1:].values # all row, and from second col to last -> skip label in zero column\n",
    "print(images.dtype)\n",
    "images = images.astype(np.float)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "image_width => 28\n",
      "image_height => 28\n"
     ]
    }
   ],
   "source": [
    "image_size = images.shape[1] #784 variables\n",
    "print(image_size)\n",
    "\n",
    "# in this case all images are square\n",
    "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "print ('image_width => {0}\\nimage_height => {1}'.format(image_width,image_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181d889198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display image\n",
    "def display(img):\n",
    "    \n",
    "    # (784) => (28,28)\n",
    "    one_image = img.reshape(image_width,image_height)\n",
    "    \n",
    "    plt.axis('off') #remove axis\n",
    "    plt.imshow(one_image, cmap=cm.binary) #cmap=cm.binary -> display black/white image\n",
    "\n",
    "# output image     \n",
    "display(images[IMAGE_TO_DISPLAY])\n",
    "display(images[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_flat(59999)\n",
      "labels_count => 10\n"
     ]
    }
   ],
   "source": [
    "labels_flat = data.iloc[:,0].values.ravel() # make it array\n",
    "print('labels_flat({0})'.format(len(labels_flat)))\n",
    "\n",
    "labels_count = np.unique(labels_flat).shape[0] # have 10 numbers in the label: 0-9\n",
    "print('labels_count => {0}'.format(labels_count)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels from scalars to one-hot vectors\n",
    "\n",
    "* A one-hot vector is a vector that contains a single element equal to 1 and the rest of the elements equal to 0. <br\\> \n",
    "Ex: 0 => [1 0 0 0 0 0 0 0 0 0] <br\\>\n",
    "    1 => [0 1 0 0 0 0 0 0 0 0] <br\\>\n",
    "    ...                        <br\\>\n",
    "    9 => [0 0 0 0 0 0 0 0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(59999,10)\n",
      "labels[10] => [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# convert class labels from scalars to one-hot vectors\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes # arange -> create array\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "labels = labels.astype(np.uint8)\n",
    "\n",
    "print('labels({0[0]},{0[1]})'.format(labels.shape))\n",
    "print ('labels[{0}] => {1}'.format(IMAGE_TO_DISPLAY,labels[IMAGE_TO_DISPLAY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(57999,784)\n",
      "validation_images(2000,784)\n"
     ]
    }
   ],
   "source": [
    "# split data into training & validation\n",
    "validation_images = images[:VALIDATION_SIZE] # we select 0:2000 as test; we set validation size=2000 (VALIDATION_SIZE = 2000)\n",
    "validation_labels = labels[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = images[VALIDATION_SIZE:] # we select 2000-42000 as train\n",
    "train_labels = labels[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Neural Nework Structure\n",
    "---\n",
    "Neural Network Process: <br/>\n",
    "image -> convolution -> max pooling -> convolution -> max pooling -> fully connected -> fully connected -> classifier\n",
    "\n",
    "---\n",
    "Ref:\n",
    "* https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_convolutional_neural_networks_work.html\n",
    "* http://cs231n.github.io/\n",
    "* https://www.youtube.com/watch?v=tjcgL5RIdTM&list=PLXO45tsB95cKI5AIlf5TxxFPzb-0zeVZ8&index=26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Things should notice for Weight initialization:\n",
    "\n",
    "Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. \n",
    "A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the “best guess” in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.\n",
    "Therefore, **we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking.**\n",
    "\n",
    "* what is weight: http://cs231n.github.io/neural-networks-1/#nn\n",
    "* concept of weight and bias: y=ax+b (a: weight; b: bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1) # create normal distribution\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolution: https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-04-CNN2/\n",
    "def conv2d(x, W):\n",
    "    # x: all inputs; W: weight\n",
    "    # strides[1,x_movement,y_movement,1]: the output is the same size as the input\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pooling : extract the maximum value\n",
    "# [[0,3],[4,2]] => 4\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # strides=[1,2,2,1] -> make 2(1+1_xmovement)*2(1+1_ymovement) into 1*1 -> squize the image \n",
    "    # ksize: extract info from 2*2 \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input & output of NN\n",
    "\n",
    "# images\n",
    "x = tf.placeholder('float', shape=[None, image_size]) #image_siae=784 (28*28)\n",
    "# labels\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])\n",
    "\n",
    "# change the shape of x (784 -> 28*28)\n",
    "image = tf.reshape(x, [-1,image_width , image_height,1]) # -1: n_sample 1: our display image is in black/white "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv1 layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) # patch:5*5; in size:1; out size:32\n",
    "b_conv1 = bias_variable([32])\n",
    "image = tf.reshape(x, [-1,image_width , image_height,1]) # =>(59999,28,28,1)\n",
    "h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1) # hidden layer convl; tf.nn.relu: make it non-linear \n",
    "# outpt size of h_convl will be 28*28*32\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "# output size of h_pool will be 14*14*32\n",
    "\n",
    "\n",
    "# Prepare for visualization\n",
    "# display 32 fetures in 4 by 8 grid\n",
    "layer1 = tf.reshape(h_conv1, (-1, image_height, image_width, 4 ,8))  \n",
    "\n",
    "# reorder so the channels are in the first dimension, x and y follow.\n",
    "layer1 = tf.transpose(layer1, (0, 3, 1, 4,2))\n",
    "layer1 = tf.reshape(layer1, (-1, image_height*4, image_width*8)) \n",
    "\n",
    "\n",
    "# conv2 layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64]) #patch:5*5; in size(width):32; out size(height):64\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # hidden layer convl; tf.nn.relu: make it non-linear \n",
    "# outpt size of h_convl will be 14*14*64\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "# output size of h_pool will be 7*7*64\n",
    "\n",
    "\n",
    "# Prepare for visualization\n",
    "# display 64 fetures in 4 by 16 grid\n",
    "layer2 = tf.reshape(h_conv2, (-1, 14, 14, 4 ,16))  \n",
    "\n",
    "# reorder so the channels are in the first dimension, x and y follow.\n",
    "layer2 = tf.transpose(layer2, (0, 3, 1, 4,2))\n",
    "\n",
    "layer2 = tf.reshape(layer2, (-1, 14*4, 14*16)) \n",
    "\n",
    "\n",
    "\n",
    "# func1 layer: hidden layer\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) # [n_sample, 7,7,64] -> [n_sample, 7*7*64]\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) # dropout: avoid overfit, apply this b4 readout layer\n",
    "\n",
    "\n",
    "# func2 layer: hidden layer (output layer輸出層)\n",
    "W_fc2 = weight_variable([1024, labels_count]) #labels_count: contains 0-9\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) # use softmax to calculate probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "#cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "#[0.1, 0.9, 0.2, 0.1, 0.1 0.3, 0.5, 0.1, 0.2, 0.3] => 1\n",
    "predict = tf.argmax(y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start TensorFlow session\n",
    "init = tf.global_variables_initializer()\n",
    "#init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 0.04 / 0.10 for step 0\n",
      "training_accuracy / validation_accuracy => 0.16 / 0.08 for step 1\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 2\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.08 for step 3\n",
      "training_accuracy / validation_accuracy => 0.04 / 0.08 for step 4\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 5\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 6\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.08 for step 7\n",
      "training_accuracy / validation_accuracy => 0.12 / 0.08 for step 8\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.08 for step 9\n",
      "training_accuracy / validation_accuracy => 0.12 / 0.08 for step 10\n",
      "training_accuracy / validation_accuracy => 0.16 / 0.08 for step 20\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 30\n",
      "training_accuracy / validation_accuracy => 0.14 / 0.08 for step 40\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 50\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 60\n",
      "training_accuracy / validation_accuracy => 0.12 / 0.08 for step 70\n",
      "training_accuracy / validation_accuracy => 0.10 / 0.08 for step 80\n",
      "training_accuracy / validation_accuracy => 0.04 / 0.08 for step 90\n",
      "training_accuracy / validation_accuracy => 0.16 / 0.08 for step 100\n",
      "training_accuracy / validation_accuracy => 0.08 / 0.08 for step 200\n",
      "training_accuracy / validation_accuracy => 0.12 / 0.08 for step 300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2c8debfea662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mdisplay_step\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# train on batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# visualisation variables\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_images[0:BATCH_SIZE], \n",
    "                                                            y_: validation_labels[0:BATCH_SIZE], \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final accuracy on validation set  \n",
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_images, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.7)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
